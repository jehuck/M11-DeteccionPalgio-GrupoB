{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import generate\n",
    "from cluster import cluster_ip\n",
    "from lxml import etree\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import operator\n",
    "import functools\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.colors as mc\n",
    "import nltk\n",
    "import numpy as np\n",
    "import textstat\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "#nltk.install(\"all\")\n",
    "\n",
    "from lexicalrichness import LexicalRichness\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import nlp\n",
    "\n",
    "from segmentador import Segmentador\n",
    "from tp2_modulos import CleanText\n",
    "\n",
    " # Feature Selection\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se elimino carpeta vieja de fragmentos sospechosos\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\sospechoso\\sospsuspicious-document00001.txt\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\sospechoso\\sospsuspicious-document00002.txt\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\sospechoso\\sospsuspicious-document00003.txt\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\sospechoso\\sospsuspicious-document00004.txt\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\sospechoso\\sospsuspicious-document00005.txt\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\sospechoso\\sospsuspicious-document00006.txt\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\sospechoso\\sospsuspicious-document00007.txt\n",
      "Se elimino carpeta vieja de fragmentos sospechosos en formato xml\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\xml\\output_xml\\suspicious-document00001.xml\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\xml\\output_xml\\suspicious-document00002.xml\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\xml\\output_xml\\suspicious-document00003.xml\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\xml\\output_xml\\suspicious-document00004.xml\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\xml\\output_xml\\suspicious-document00005.xml\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\xml\\output_xml\\suspicious-document00006.xml\n",
      "Eliminado: c:\\Users\\pbonafe\\Diplo\\mentoria\\M11-DeteccionPlagio-GrupoB\\xml\\output_xml\\suspicious-document00007.xml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "actual_directory = os.getcwd()\n",
    "\n",
    "ruta = os.path.join(actual_directory, \"mini_corpus\")\n",
    "ruta_del_directorio = str(ruta) + \"\\\\*.txt\"\n",
    "# Me produce una lista con los nombres de los archivos txt del corpus\n",
    "textos_originales = glob.glob(ruta_del_directorio)\n",
    "\n",
    "\n",
    "if not os.path.exists(str(actual_directory)+\"\\\\cleaned_corpus\"):\n",
    "    os.mkdir(str(actual_directory)+'\\\\cleaned_corpus')\n",
    "    \n",
    "cleaned_corpus = str(actual_directory)+\"\\\\cleaned_corpus\"\n",
    "\n",
    "# De esta manera le voy robo el nombre a los xml\n",
    "path_xml = str(actual_directory) + \"\\\\mini_corpus\" + \"\\\\*.xml\"\n",
    "xml_docs = glob.glob(path_xml)\n",
    "\n",
    "\n",
    "if not os.path.exists(str(actual_directory)+\"\\\\sospechoso\"):\n",
    "    os.mkdir(str(actual_directory)+'\\\\sospechoso')\n",
    "else:\n",
    "    print(\"Se elimino carpeta vieja de fragmentos sospechosos\")\n",
    "    patron_inicio = 'sosp*'\n",
    "    patron_final = '*.txt'\n",
    "    archivos = os.listdir(str(actual_directory)+\"\\\\sospechoso\")\n",
    "    for archivo in archivos:\n",
    "        if fnmatch.fnmatch(archivo, patron_inicio) and fnmatch.fnmatch(archivo, patron_final):\n",
    "            ruta_completa = os.path.join(str(actual_directory)+\"\\\\sospechoso\", archivo)\n",
    "            os.remove(ruta_completa)\n",
    "            print(f\"Eliminado: {ruta_completa}\")\n",
    "\n",
    "    \n",
    "if not os.path.exists(str(actual_directory) + \"\\\\xml\\\\output_xml\"):\n",
    "    os.mkdir(str(actual_directory) + \"\\\\xml\\\\output_xml\")\n",
    "else:\n",
    "    print(\"Se elimino carpeta vieja de fragmentos sospechosos en formato xml\")\n",
    "    patron_final = '*.xml'\n",
    "    archivos = os.listdir(str(actual_directory) + \"\\\\xml\\\\output_xml\")\n",
    "    for archivo in archivos:\n",
    "        if fnmatch.fnmatch(archivo, patron_final):\n",
    "            ruta_completa = os.path.join(str(actual_directory) + \"\\\\xml\\\\output_xml\", archivo)\n",
    "            os.remove(ruta_completa)\n",
    "            print(f\"Eliminado: {ruta_completa}\")\n",
    "\n",
    "\n",
    "sospechosos_path = str(actual_directory) + \"\\\\sospechoso\\\\\"\n",
    "output_xml = str(actual_directory) + \"\\\\xml\\\\output_xml\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 1\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 38}\n",
      "File: 2\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 271}\n",
      "File: 3\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 1\n",
      "{0: 58, -1: 1}\n",
      "  \n",
      "58\n",
      "58\n",
      "Case 1, y == (last + 1)\n",
      "Entro2\n",
      "File: 4\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 22}\n",
      "File: 5\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 30}\n",
      "File: 6\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 228}\n",
      "File: 7\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 7}\n",
      "File: 8\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 1\n",
      "{0: 869, -1: 1}\n",
      "  \n",
      "869\n",
      "869\n",
      "Case 1, y == (last + 1)\n",
      "Entro2\n",
      "File: 9\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 8}\n",
      "File: 10\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 586}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Comienzo a leer el conjunto de textos\n",
    "for i, archive in enumerate(textos_originales):\n",
    "    \n",
    "    print(\"File: \" + str(i+1))\n",
    "    \n",
    "    f = open(archive, encoding=\"utf-8-sig\")\n",
    "    # Leo contenido del archivo\n",
    "    text = f.read()\n",
    "\n",
    "    # Cierro el txt\n",
    "    f.close()           \n",
    "    \n",
    "    # Nombre de Archivo\n",
    "    filename = os.path.basename(archive)\n",
    "    # Separo nombre de archivo y extension\n",
    "    filename2, extension = os.path.splitext(filename)\n",
    "    \n",
    "    Seg = Segmentador(text)\n",
    "    # La variable data_text posee el texto completo, separado en párrafos\n",
    "    data_text = Seg.content_segmentation()\n",
    "    \n",
    "\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    lista_to_df = []\n",
    "    \n",
    "    for j, segmento in enumerate(data_text):\n",
    "\n",
    "        segmento_limpio = CleanText(str(segmento)).lemmatizeText()\n",
    "\n",
    "        data_list.append([j, re.sub(\"\\n\", \" \", segmento_limpio), len(re.sub(\"\\n\", \" \", segmento_limpio))])\n",
    "\n",
    "        lista_to_df.append(\n",
    "            [j, filename, nlp.getnumOfPunctN(segmento_limpio), nlp.gettypeToken(segmento_limpio)]\n",
    "        )                        \n",
    "    \n",
    "    df = pd.DataFrame(lista_to_df, columns=[\"index\", \"filename\", \"getnumOfPunctN\", \"gettypeToken\"])\n",
    "    \n",
    "    \n",
    "    #df[\"getfleshReadingEase\"] = df[\"getfleshReadingEase\"] / df[\"getfleshReadingEase\"].max()\n",
    "    \n",
    "    # Seleccionar los valores numéricos del df\n",
    "    # Este paso se realiza para ir viendo que genera cada párrafo del txt y como fue convertido a un vector\n",
    "    dataframe = df.iloc[:,[2,3]]\n",
    "    \n",
    "    # Aprendizaje No Supervisado\n",
    "    cluster = cluster_ip(dataframe, 4, \"cosine\")\n",
    "    labels = cluster.labels_    \n",
    "    df[\"labels\"] = labels\n",
    "    \n",
    "    realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    clusterNum=len(set(labels))\n",
    "    n_noise = list(labels).count(-1) \n",
    "    \n",
    "    print(\"Estimated number of clusters: %d\" % realClusterNum)\n",
    "    print(\"Estimated number of noise points: %d\" % n_noise)\n",
    "    counts = df[\"labels\"].value_counts().to_dict()\n",
    "    print(counts)\n",
    "    \n",
    "    # Me sirve para guardar los casos de plagio y compararlos\n",
    "    xml_name = os.path.basename(xml_docs[i])\n",
    "    xml_name2, extension = os.path.splitext(xml_name)\n",
    "    \n",
    "    # Se crea el archivo txt donde se guardaran los fragmentos sospechosos\n",
    "    sherlock = codecs.open(sospechosos_path + \"sosp\" + filename,\"w\",\"utf-8\")\n",
    "    \n",
    "    # Los fragmentos que fueron clasificados como plagio\n",
    "    # Se guardan en el archivo .txt\n",
    "    # Tener en cuenta que el dataframe y la lista data_list, manejan los mismos índices\n",
    "    # Por eso, si en el dataframe se encuentra un \"-1\" es porque este es un outlier\n",
    "    for x in range(len(df[\"labels\"])):\n",
    "        l = df[\"labels\"][x]\n",
    "        if l == -1:\n",
    "            print(data_list[x][1])\n",
    "            print(x)\n",
    "            sherlock.write(data_list[x][1]+os.linesep)       \n",
    "               \n",
    "    sherlock.close()    \n",
    "    \n",
    "    document_el = etree.Element(\"document\", reference = xml_name2 + \".txt\")\n",
    "\n",
    "    offset = []\n",
    "\n",
    "    init = 0\n",
    "\n",
    "    sum_length = []\n",
    "\n",
    "    last = -1\n",
    "\n",
    "    for y in range(len(df[\"labels\"])):       \n",
    "\n",
    "        l = df[\"labels\"][y]\n",
    "\n",
    "        if l == -1:\n",
    "\n",
    "            print(y)\n",
    "\n",
    "            if y == 0:\n",
    "\n",
    "                sum_length.append([init, data_list[y][2]])\n",
    "\n",
    "                print(f\"Case 0, y = 0\")\n",
    "\n",
    "            elif y != 0 and (y == (last + 1) or len(sum_length) == 0):\n",
    "\n",
    "                sum_length.append([init, data_list[y][2]])\n",
    "\n",
    "                print(f\"Case 1, y == (last + 1)\")\n",
    "\n",
    "            elif y != 0 and y != (last + 1):\n",
    "\n",
    "                this_length = sum([l[1] for l in sum_length])\n",
    "\n",
    "                feature_el = etree.SubElement(document_el, \"feature\", name=\"detected-plagiarism\", this_offset=str(sum_length[0][0]), this_length=str(this_length))\n",
    "\n",
    "                sum_length.clear()\n",
    "\n",
    "                print(sum_length)\n",
    "\n",
    "                sum_length.append([init, data_list[y][2]])\n",
    "\n",
    "                print(f\"Case 2, y != (last + 1)\")   \n",
    "\n",
    "            last = y\n",
    "\n",
    "        if y == (len(df[\"labels\"]) - 1) and len(sum_length) > 0:\n",
    "\n",
    "            print(\"Entro2\")\n",
    "\n",
    "            this_length = sum([l[1] for l in sum_length])\n",
    "\n",
    "            feature_el = etree.SubElement(document_el, \"feature\", name=\"detected-plagiarism\", this_offset=str(sum_length[0][0]), this_length=str(this_length))\n",
    "\n",
    "\n",
    "        init += data_list[y][2] + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    et = etree.ElementTree(document_el)\n",
    "    with open(output_xml+xml_name, \"wb\") as fe:\n",
    "        et.write(fe, encoding=\"utf-8\", xml_declaration=None, pretty_print=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
